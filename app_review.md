# App Review: TTS-Proof

## 1. How the App is Supposed to Work (Design)

The application is designed as a sophisticated grammar and spelling correction tool for Markdown files, leveraging the power of local Large Language Models (LLMs). Its primary goal is to improve the quality of text, particularly for content that may have been transcribed, translated, or generated by other AI systems. The core design principles are:

*   **Local First:** The application is built to work with locally-hosted LLMs (via servers like LM Studio), ensuring user privacy and eliminating reliance on cloud services.
*   **Markdown-Aware:** It's specifically designed to handle Markdown files, preserving the structural elements like headers, lists, code blocks, and links, while only correcting the plain text content.
*   **Two-Pass Correction (Prepass + Grammar):** The system employs a two-stage process for enhanced accuracy:
    1.  **Prepass:** A specialized first pass to detect and normalize text that is particularly problematic for Text-to-Speech (TTS) engines. This includes issues like stylized Unicode characters, excessive spacing, and other formatting anomalies.
    2.  **Grammar Correction:** The main pass where the LLM corrects grammar, spelling, and punctuation errors, improving the overall readability of the text. The results of the prepass are injected into the grammar prompt to ensure that the initial corrections are preserved.
*   **Web-Based UI and CLI:** The application is designed to be accessible through both a modern, user-friendly web interface and a powerful command-line interface (CLI) for batch processing and automation.
*   **Checkpointing and Resumability:** The design includes a robust checkpointing system that allows users to resume lengthy processing tasks from where they left off, preventing data loss in case of interruptions.

## 2. How the App Currently Works (Implementation)

The application's current implementation is a full-stack solution with a React/TypeScript frontend and a Python/FastAPI backend. The core logic is encapsulated in Python scripts that are shared between the backend and the CLI.

*   **Backend (`backend/app.py`):** A FastAPI server that exposes the core functionality through a REST API and WebSockets.
    *   **API Endpoints:** It provides endpoints for file uploads, model selection, running the prepass and grammar correction processes, and managing prompts.
    *   **WebSockets:** WebSockets are used to provide real-time progress updates to the frontend during long-running processing tasks.
    *   **Asynchronous Processing:** The backend uses `asyncio` and `concurrent.futures` to run the LLM calls in separate threads, preventing the server from being blocked during intensive processing.
*   **CLI (`md_proof.py`):** A powerful command-line tool that provides access to the full range of the application's features.
    *   **Feature Rich:** It supports checkpointing, resuming, a "prepass" mode, and various other options for fine-tuning the processing.
    *   **Shared Logic:** It shares the core markdown processing and LLM interaction logic with the backend, ensuring consistency between the two interfaces.
*   **Core Logic (`prepass.py` and `md_proof.py`):**
    *   **Chunking:** The input Markdown is intelligently chunked into smaller pieces, separating text from code blocks to ensure that only the text is sent to the LLM for correction.
    *   **Prompt Engineering:** The application's effectiveness heavily relies on carefully engineered prompts that are loaded from external files (`prepass_prompt.txt` and `grammar_promt.txt`). These prompts have been iteratively optimized to produce the best results with the supported LLMs.
    *   **LLM Interaction:** The `call_lmstudio` function handles the communication with the local LLM server, sending the prompts and the text to be corrected, and then parsing the response.
*   **Frontend (`frontend/`):** A modern React application built with TypeScript and Vite.
    *   **User-Friendly Interface:** It provides a six-section grid layout for a logical workflow, allowing users to select a file, configure the model, and then run the correction process.
    *   **Real-time Updates:** It uses WebSockets to display real-time progress updates, providing a much better user experience than a traditional request-response model.

## 3. Test Results and Progression

The project has a comprehensive testing strategy that includes unit tests, stress tests, and simple integration tests. The test results, as documented in `testing/docs/`, show a clear progression of the application's capabilities.

*   **Initial State:** The initial versions of the application had a baseline performance of 0.0% "Reference Match" (a metric used to gauge the quality of the corrections).
*   **Prompt Optimization:** Through a series of iterative tests, the prompts were significantly improved, leading to a dramatic increase in the Reference Match score to 15.0%. This was achieved by:
    *   **JSON-First Approach:** Forcing the LLM to return JSON in the prepass stage to eliminate parsing errors.
    *   **Specific Examples:** Providing clear examples in the prompts to guide the LLM's behavior.
    *   **Clear Boundaries:** Explicitly telling the LLM what to skip (e.g., code blocks, URLs).
*   **Model Tuning:** The testing process also involved experimenting with different LLMs and their parameters. For example, it was discovered that disabling the "thinking" mode of the Qwen 3 8B model resulted in a significant speed improvement without sacrificing quality.
*   **Current State:** The application is now in a much more stable and effective state, with a 15.0% Reference Match score and a 25% speed improvement over the baseline. The testing infrastructure is well-established, allowing for rapid iteration and improvement.

## 4. What Needs to be Done (Hypothesis for the Next Iteration)

Based on the current state of the application and the test results, here are some hypotheses for the next iteration:

*   **The 15% Quality Ceiling:** The test results suggest that the current approach may have reached a quality ceiling of 15% Reference Match. To break through this, the next iteration should focus on:
    *   **Grammar Prompt Optimization:** The grammar correction step contributes the most to the overall quality (12% of the 15%), so focusing on optimizing the grammar prompt is likely to yield the best results. This could involve experimenting with different temperatures, adding more grammar-specific examples, or even using a two-pass grammar correction process.
    *   **Model Upgrades:** The next logical step is to test more powerful models, such as the Qwen 3 14B or Llama 3.1 8B, which may be able to provide more nuanced and accurate corrections.
*   **User Experience Enhancements:** While the core functionality is solid, the user experience could be improved by:
    *   **Real-time Chunk Preview:** Displaying the corrected chunks as they are completed, rather than waiting for the entire process to finish, would provide a more interactive experience.
    *   **Diff View:** A side-by-side comparison of the original and corrected text, with the changes highlighted, would make it much easier for users to review the corrections.
*   **Advanced Features:**
    *   **Processing Profiles:** Allowing users to save and load different processing profiles (e.g., for different types of documents or different LLMs) would improve the application's usability.
    *   **Batch Processing:** Supporting the processing of multiple files at once would be a major feature for users who need to correct a large number of documents.

In summary, the application is a well-designed and well-implemented tool with a solid foundation. The next iteration should focus on breaking through the current quality ceiling by optimizing the grammar correction process and experimenting with more powerful models, while also enhancing the user experience with features like real-time previews and a diff view.
