# TTS-Proof - Modern Grammar Correction Tool

## ğŸ“š What Does This Do? (Simple Version)

Got a document with terrible spelling, weird formatting, or messy translation? Here's your fix:

1. **Convert your document to Markdown** using tools like [Calibre](https://calibre-ebook.com/) or [Pandoc](https://pandoc.org/)
2. **Run it through TTS-Proof** - it uses AI to clean up grammar, spelling, and odd text formatting
3. **Convert it back** to your original format (PDF, DOCX, EPUB, etc.)
4. **VoilÃ !** âœ¨ Clean, readable document ready to go

Perfect for cleaning up:
- ğŸ“– **Ebooks** with OCR errors or poor formatting
- ğŸŒ **Web articles** that were badly translated  
- ğŸ“ **Old documents** with outdated spelling conventions
- ğŸ¤– **AI-generated text** that needs polishing
- ğŸ“„ **Any text** that just looks messy and unprofessional

*No internet required - everything runs locally on your computer for privacy!*

---

## Overview

A modern web-based application for batch grammar and spelling correction of Markdown files using **local LLM** servers (LM Studio). Features a beautiful React frontend with optimized layout, real-time progress tracking, and a robust FastAPI backend that preserves all original processing capabilities.

## ğŸ“¸ Interface Preview

![TTS-Proof Web Interface](screenshot-web-interface.png)

*Modern 6-section grid layout optimized for wide displays: organized workflow from file selection to processing, with preview and logs at the bottom.*

---

## âœ¨ Features

### ğŸ¨ Modern Web Interface
- **Optimized 6-section grid layout** perfect for 4K displays and wide screens
- **Logical workflow organization** - file setup â†’ model config â†’ processing controls
- **React + TypeScript frontend** with Tailwind CSS styling  
- **Real-time progress updates** via WebSocket connections
- **Dark/Light theme toggle** for comfortable use
- **Drag & drop file uploads** with instant preview and file analysis

### ğŸ”§ Powerful Processing Engine
- **TTS Prepass Detection** with auto-selection for grammar correction workflow
- **Flexible LLM server support** with configurable endpoints (LM Studio, KoboldCpp, Oobabooga, TabbyAPI, etc.)
- **Network-ready** - connect to local or remote servers on your network
- **Intelligent chunking** with configurable sizes and progress tracking
- **Crash-safe processing** with automatic checkpointing and resume
- **Markdown structure preservation** (headings, lists, links, code blocks)
- **URL masking** and code-block protection during processing

### ğŸš€ Advanced Architecture
- **FastAPI backend** with WebSocket support for real-time updates
- **Concurrent processing** with proper job management
- **Model auto-detection** from LM Studio server
- **Customizable prompts** with live editing capabilities
- **File management** with temporary file handling and cleanup

---

## ğŸ“‹ Requirements

- **Python 3.10+**
- **Node.js 16+** and npm
- **LM Studio** with local server enabled
- Grammar-capable model (e.g., `qwen/qwen3-4b-2507`)

---

## ğŸš€ Quick Start

### 1. Setup LM Studio
1. Download and install [LM Studio](https://lmstudio.ai/)
2. Download a grammar-capable model (e.g., `qwen/qwen3-4b-2507`)
3. Enable **Local Server** with OpenAI-compatible API
4. Default URL: `http://127.0.0.1:1234/v1`

### 2. Install Dependencies

**Backend:**
```bash
cd backend
pip install fastapi uvicorn websockets python-multipart requests regex
```

**Frontend:**
```bash
cd frontend
npm install
```

### 3. Start the Application

#### ğŸš€ **Easy Launch (Recommended)**

Use one of the provided launchers to start both servers with a single command:

**Windows (Batch):**
```cmd
launch.bat
```

**Windows (PowerShell):**
```powershell
.\launch.ps1
```

**Cross-platform (Python):**
```bash
python launch.py
```

**Unix/Linux/macOS (Shell):**
```bash
./launch.sh
```

#### ğŸ“‹ **Manual Launch**

If you prefer to start servers separately:

**Terminal 1 - Backend:**
```bash
cd backend
python app.py
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm run dev
```

**Access:** Open `http://localhost:5174` in your browser

### ğŸ® **Launcher Features**

The provided launchers automatically:
- âœ… **Check system requirements** (Python 3.10+, Node.js 16+)
- âœ… **Install missing dependencies** (pip and npm packages)
- âœ… **Start both servers** simultaneously
- âœ… **Open your browser** to the application
- âœ… **Handle cleanup** when stopping (Unix/Linux/Python launcher)
- âœ… **Provide clear status updates** and error messages

Choose the launcher that works best for your system!

---

## ğŸ¯ How to Use

### Streamlined 6-Section Workflow:

**Top Row - Setup:**
1. **File Selection**: Drag & drop your Markdown file with instant analysis
2. **Model Selection**: Choose your LLM model and configure endpoints
3. **Chunk Size**: Set optimal processing chunk size (4K-16K chars)

**Bottom Row - Processing:**
4. **TTS Prepass**: Run detection for TTS-problematic words (auto-selects for correction)
5. **Prompt Template**: Customize grammar correction instructions
6. **Processing**: Execute with real-time progress tracking

**Preview & Logs**: Monitor results and processing details at the bottom

### Quick Steps:
1. **Upload** your file â†’ **Select** model â†’ **Process** â†’ **Download** results
2. Optional: Run **TTS Prepass** for enhanced correction of speech-problematic text

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    WebSocket    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚  FastAPI Backend â”‚
â”‚  (Port 5174)    â”‚                 â”‚  (Port 8000)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                             â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚   LM Studio      â”‚
                                    â”‚  (Port 1234)     â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Backend Features
- FastAPI server with WebSocket support
- Original `md_proof.py` processing logic integration
- Real-time progress updates and job management
- File upload handling and temporary file management

### Frontend Features
- React 18 with TypeScript and optimized 6-section grid layout
- Tailwind CSS for modern styling with 4K display optimization
- Real-time WebSocket communication with live progress updates
- TTS Prepass integration with auto-selection workflow
- Theme switching and responsive design for all screen sizes

---

## ğŸŒ Server Configuration

### Configuring LLM Server Endpoints

TTS-Proof supports multiple LLM server backends with configurable endpoints:

1. **Click the edit button** (ğŸ“) next to the model selection dropdown
2. **Choose from presets:**
   - **LM Studio**: `http://127.0.0.1:1234/v1` (default)
   - **KoboldCpp**: `http://127.0.0.1:5001/v1`
   - **Oobabooga**: `http://127.0.0.1:5000/v1`
   - **TabbyAPI**: `http://127.0.0.1:5000/v1`

3. **Or configure custom endpoints:**
   - Enter custom IP address and port
   - Full URL configuration support
   - Network server support (e.g., `http://192.168.1.100:1234/v1`)

### Remote Server Setup
To connect to a server on another machine:
1. Ensure the server accepts connections from your IP
2. Use the custom endpoint option
3. Enter the server's IP address and port
4. Test connection by refreshing models

---

## ğŸ”§ Advanced Features

### Command Line Interface (Legacy)
The original CLI is still available for batch processing:
```bash
python md_proof.py document.md --stream --preview-chars 400
```

### Prompt Customization
- Edit prompts directly in the web interface
- Changes are saved to `grammar_promt.txt` automatically
- Live preview of prompt changes

### EPUB Integration
Convert EPUB files for processing:
```bash
# EPUB to Markdown
pandoc input.epub -t gfm -o book.md

# Process with TTS-Proof web interface

# Markdown back to EPUB
pandoc book.corrected.md -o corrected.epub
```

---

## ğŸ“Š Performance

- **Tested:** RTX 2060 (8GB VRAM) - 40 minutes for 300+ page document
- **Chunking:** 8000 character chunks for optimal processing
- **Checkpointing:** Automatic resume on interruption
- **Memory:** Efficient streaming prevents memory issues

---

## ğŸ› ï¸ Development

### Project Structure
```
tts-proof/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py              # FastAPI server
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â”œâ”€â”€ services/       # API services
â”‚   â”‚   â””â”€â”€ App.tsx         # Main application
â”‚   â”œâ”€â”€ package.json        # Node.js dependencies
â”‚   â””â”€â”€ vite.config.ts      # Vite configuration
â”œâ”€â”€ md_proof.py             # Core processing logic
â””â”€â”€ grammar_promt.txt       # Grammar correction prompts
```

### Building for Production
```bash
# Frontend build
cd frontend
npm run build

# Backend deployment
cd backend
pip install -r requirements.txt
uvicorn app:app --host 0.0.0.0 --port 8000
```

---

## ï¿½ TODO / Future Enhancements

### ğŸ¯ UI/UX Improvements
- [ ] **Separate model picker for prepass** - Allow different models for TTS detection vs grammar correction
- [ ] **Expose prepass prompt in web UI** - Make TTS detection prompts editable like grammar prompts
- [ ] **Reorganize web UI layout** - Improve component arrangement for more intuitive workflow
- [ ] **Real-time chunk preview** - Display processed chunks as they complete, not just final result
- [ ] **Open temp files location button** - Quick access to temporary/processing files directory in file browser
- [ ] **Batch file processing** *(optional)* - Support multiple file uploads and queue management
- [ ] **Processing history** *(optional)* - Keep track of recently processed files and settings

### ğŸ”§ Core Features *(optional enhancements)*
- [ ] **Custom prompt templates** - Save and switch between different correction strategies
- [ ] **Processing profiles** - Quick presets for different document types (academic, creative, technical)
- [ ] **Diff view** - Side-by-side comparison showing original vs corrected text with highlights
- [ ] **Export options** - Support for different output formats (PDF, DOCX, plain text)
- [ ] **Undo/redo functionality** - Allow users to revert or modify corrections
- [ ] **Smart resume** - Better checkpoint recovery with partial chunk restoration

### ğŸš€ Advanced Features *(optional enhancements)*
- [ ] **Model performance analytics** - Track processing speed and quality metrics per model
- [ ] **API rate limiting** - Configurable delays between LLM calls to prevent server overload
- [ ] **Collaborative editing** - Multi-user support with real-time synchronization
- [ ] **Plugin system** - Extensible architecture for custom processing rules
- [ ] **Cloud deployment** - Docker containerization and deployment guides
- [ ] **Mobile responsiveness** - Optimize interface for tablet and mobile devices

### ğŸ§ª Quality & Testing *(optional enhancements)*
- [ ] **Automated testing suite** - Comprehensive unit and integration tests
- [ ] **Performance benchmarking** - Measure and optimize processing speeds
- [ ] **Error recovery** - Better handling of network interruptions and server failures
- [ ] **Logging improvements** - More detailed debugging and audit trails

*Contributions welcome! Feel free to tackle any of these items or suggest new features.*

---

## ï¿½ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| **Models not loading** | Ensure LM Studio server is running on port 1234 |
| **WebSocket connection failed** | Check if backend is running on port 8000 |
| **Frontend won't start** | Run `npm install` in frontend directory |
| **Processing stuck** | Check LM Studio logs, restart if needed |
| **File upload failed** | Ensure file is .md, .txt, or .markdown format |

---

## ğŸ“„ License

Personal utility tool - adapt and use as you wish. No warranty provided.

## ğŸ¤ Contributing

Feel free to submit issues, feature requests, or pull requests to improve the tool.
